{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Tennis: Collaboration and Competition\n",
    "\n",
    "* Implement a multi-agent DDPG algorithm to train two agents to play a game of tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.20 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the necessary libraries and software modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import sys\n",
    "sys.path.insert(0, '/home/workspace/python/src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load 'Tennis' Unity Environment\n",
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for both agents look like: [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "   6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.4669857  -1.5         0.          0.\n",
      "  -6.83172083  6.          0.          0.        ]]\n",
      "The rewards for both agents look like: [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for both agents look like:', states)\n",
    "\n",
    "rewards = env_info.rewards\n",
    "print('The rewards for both agents look like:', rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Global Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the RNG for reproducability\n",
    "SEED = 1\n",
    "\n",
    "# Select GPU for training if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Class Definitions\n",
    "* Define various classes directly in the notebook for editing convenience\n",
    "* Found that updating logic in external python files was not getting immediately reflected in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Action Space Noise and Replay Buffer\n",
    "* Implement action space noise via an Ornstein-Uhlenbeck process for policy exploration\n",
    "* Create a buffer to store experience tuples and sample uncorrelated batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize noise parameters\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(SEED)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state to mean.\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample\"\"\"\n",
    "        x =self.state\n",
    "        noise = self.theta * (self.mu - x) + self.sigma * (np.random.randn(*x.shape)-0.5)\n",
    "        self.state = self.theta + noise\n",
    "        return self.state\n",
    "\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(SEED)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store new experience to replay memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory for training\"\"\"\n",
    "        experiences =  random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Define Actor and Critic Network Architecture\n",
    "* DDPG utilizes an Actor/Critic network architecture to estimate the optimal policy/value functions\n",
    "* Adapted from Pendulum model in the Udacity Deep Reinforcement Learning Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UNIFORM_BOUND = 5e-3     # bounds for uniform distribution \n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1.0/np.sqrt(fan_in)\n",
    "    return (-lim,lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-UNIFORM_BOUND, UNIFORM_BOUND)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-UNIFORM_BOUND, UNIFORM_BOUND)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Define DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "LEARN_EVERY = 10        # learning timestep interval\n",
    "NUM_LEARNS = 5         # number of learning passes\n",
    "UPDATE_EVERY = 1        # update timestep interval\n",
    "NUM_UPDATES = 1         # number of updates each interval\n",
    "NOISE_VAR = 1.0         # amplitude of action space noise\n",
    "NOISE_REDUCTION = 1e-5  # reduction rate of action noise\n",
    "\n",
    "class DDPG_Agent():   \n",
    "    def __init__(self, state_size, action_size, num_agents=2):\n",
    "        \"\"\"Initialize a DDPG Agent object.\"\"\"\n",
    "        self.seed = random.seed(SEED)\n",
    "        self.epsilon = NOISE_VAR\n",
    "        \n",
    "        # Initialize the local and target Actor network\n",
    "        self.actor_local = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Initialize the local and target Critic network\n",
    "        self.critic_local = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
    "        \n",
    "        # Initialize the Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        # Initialize the Ornsten-Uhlenbeck process to add exploration noise\n",
    "        self.noise = OUNoise((num_agents,action_size))\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "        \n",
    "    def step(self, S, A, R, NS, T, t_i):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(S, A, R, NS, T)\n",
    "\n",
    "        # Learn at specified interval, if enough samples are available in memory\n",
    "        # -> option to learn multiple times per interval\n",
    "        if len(self.memory) > BATCH_SIZE and t_i % LEARN_EVERY == 0:\n",
    "            for _ in range(NUM_LEARNS):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, t_i)\n",
    "                \n",
    "        if NOISE_REDUCTION > 0:\n",
    "            self.epsilon -= NOISE_REDUCTION\n",
    "                \n",
    "                \n",
    "    def act(self, S, explore=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        S = torch.from_numpy(S).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(S).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if explore:\n",
    "            action += self.epsilon * self.noise.sample()    \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target_model = TAU*θ_local_model + (1 - TAU)*θ_target_model\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "        \n",
    "    def learn(self, experiences, t_i):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        \"\"\"\n",
    "        S, A, R, NS, T = experiences\n",
    "        \n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        NA = self.actor_target(NS)\n",
    "        Q_targets_next = self.critic_target(NS, NA)\n",
    "        \n",
    "        # compute Q-value targets\n",
    "        Q_targets = R + (GAMMA * Q_targets_next * (1 - T))\n",
    "        \n",
    "        # compute critic loss\n",
    "        Q_expected = self.critic_local(S, A)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # compute actor loss\n",
    "        A_pred = self.actor_local(S)\n",
    "        actor_loss = -self.critic_local(S, A_pred).mean()\n",
    "        \n",
    "        # minimize actor loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        if (t_i % UPDATE_EVERY == 0):\n",
    "            for _ in range(NUM_UPDATES):\n",
    "                self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "                self.soft_update(self.actor_local, self.actor_target, TAU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize DDPG Agent Object\n",
    "* Show internal network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=258, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = DDPG_Agent(state_size, action_size) # instantiate the DDPG agent\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Training Loop\n",
    "* Specify constraints for agent training:\n",
    "    * Max episodes\n",
    "    * Max episode timesteps\n",
    "    * finish line score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PASSING_SCORE = 0.5      # environment solved when average score over 100 episodes is this value \n",
    "\n",
    "def train(n_episodes=2000, max_t=1000, print_every=50):\n",
    "    \n",
    "    max_scores = [] \n",
    "    avg_scores = [] \n",
    "    scores_deque = deque(maxlen=100) # project requires average over 100 episodes\n",
    "\n",
    "    # iterate over episodes\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        # iterate over timesteps each episode\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, explore=True)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            scores += rewards\n",
    "            \n",
    "            # iterate over agents in environment\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "                \n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        # Bookkeep scores each episode\n",
    "        max_scores.append(np.max(scores))\n",
    "        scores_deque.append(np.max(scores)) \n",
    "        avg_scores.append(np.mean(scores_deque))\n",
    "        \n",
    "        # print progress\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tMax Score: {:.2f}'.format(i_episode, avg_scores[-1], max_scores[-1]), end=\"\")\n",
    "        \n",
    "        # latch print at specified interval\n",
    "        if i_episode % print_every == 0:\n",
    "            print(\"\\rEpisode {}\\tAverage Score: {:.4f}\\tMax Score: {:.2f}\".format(i_episode, avg_scores[-1], max_scores[-1]))\n",
    "\n",
    "        # record final data once completion targets reached (Average 'PASSING_SCORE' for 100 consecutive episodes)\n",
    "        if avg_scores[-1] >= PASSING_SCORE and i_episode >= 100:\n",
    "            print(\"\\nEnvironment solved in {} episodes.\\tAverage score: {:.4f}\".format(i_episode, avg_scores[-1]))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_final.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_final.pth')\n",
    "            break    \n",
    "            \n",
    "    return max_scores, avg_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Agent and Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAverage Score: 0.0020\tMax Score: 0.00\n",
      "Episode 100\tAverage Score: 0.0048\tMax Score: 0.09\n",
      "Episode 150\tAverage Score: 0.0066\tMax Score: 0.00\n",
      "Episode 200\tAverage Score: 0.0028\tMax Score: 0.00\n",
      "Episode 250\tAverage Score: 0.0000\tMax Score: 0.00\n",
      "Episode 300\tAverage Score: 0.0010\tMax Score: 0.00\n",
      "Episode 350\tAverage Score: 0.0090\tMax Score: 0.00\n",
      "Episode 400\tAverage Score: 0.0166\tMax Score: 0.00\n",
      "Episode 450\tAverage Score: 0.0096\tMax Score: 0.10\n",
      "Episode 500\tAverage Score: 0.0125\tMax Score: 0.00\n",
      "Episode 550\tAverage Score: 0.0336\tMax Score: 0.10\n",
      "Episode 600\tAverage Score: 0.0366\tMax Score: 0.00\n",
      "Episode 650\tAverage Score: 0.0379\tMax Score: 0.00\n",
      "Episode 700\tAverage Score: 0.0697\tMax Score: 0.30\n",
      "Episode 750\tAverage Score: 0.2126\tMax Score: 0.10\n",
      "Episode 800\tAverage Score: 0.2842\tMax Score: 0.30\n",
      "Episode 850\tAverage Score: 0.4802\tMax Score: 0.20\n",
      "Episode 851\tAverage Score: 0.5052\tMax Score: 2.60\n",
      "Environment solved in 851 episodes.\tAverage score: 0.5052\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5281147208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from workspace_utils import active_session # prevent notebook connection from dropping during long training session\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "MAX_EPISODES = 2000      # maximum number of training episodes\n",
    "MAX_TIMESTEPS = 1000     # maximum number of timesteps per episode\n",
    "\n",
    "with active_session():\n",
    "    episode_scores, average_scores = train(MAX_EPISODES, MAX_TIMESTEPS)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(episode_scores)), episode_scores, label='Episode Score ')\n",
    "    plt.plot(np.arange(len(episode_scores)), average_scores, c='r', linestyle = '--', label='Average Score')\n",
    "    plt.title('DDPG Multi-Agent Performance')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Future Work for Performance Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for improved performance:\n",
    "* Implement parameter space noise rather than action space noise to ensure network output actions are not independent of input states\n",
    "* Use a prioritized replay buffer rather than sampling experience tuples according to a uniform distribution\n",
    "* Perform more testing to understand why there appears to be a high sensitivity to network/agent implementation methodology (like defining classes in .py files as opposed to here in the notebook. See attached Report.md) and take steps to mitigate\n",
    "* Instead of only using fully connected (linear) layers, try incorporating more complex nodes like LSTM cells to better capture temporal relationships\n",
    "* Test with other features like leaky_relu activation functions and dropout layers to see how they impact performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
